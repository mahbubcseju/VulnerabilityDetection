# Slice Level Vulnerability Detection Using Tranformer Based Model

## About 
This is our project work for the course COM S 527.

## Requirements

`python3.7`

To install other python packages, run

```
pip install -r requirements.txt 
```

## Dataset

You can either build the dataset ([following these steps](#how-to-convert-function-to-slices-and-build-our-dataset)) or directly download from the below link. But building the dataset from scratch is very costly which requires high computation supported machines.

[DOWNLOAD LINK](https://iastate.box.com/s/9jrlvc3chs70wq6ra8vkrgskifcy4se3)


After downloading the above datasets, put them inside `dataset` directory.

## Training/Fine-tuning
Before running training or testing, set the following paths in the `src/model/config.py` file .
```
PROJECT_DIR = '{}/VulnerabilityDetection/' // Path of the root directory of the project
SAVED_MODEL_DIR = 'saved_models' // Directory where the saved model are stored
DATASET_DIR = 'dataset/' // Path of the dataset directory
```

To train the function-level approach, run the following script inside   `src/model` directory.

```
    python run.py --dataset function_level
```
 
 For slice-level approach, run the following script  inside  `src/model` directory.

```
    python run.py --dataset slice_level
```

## Fine-tuned Checkpoints
We also published our fine-tuned weights checkpoints. If you dont want to train but only want to test, then download
the checkpoints from the below link and put them inside `saved_models` directory. After that, you can run the testing.

[DOWNLOAD LINK](https://iastate.box.com/s/hxtb0tv9jtnmnej3tv3qrrwo981n1qp7)


## Testing 
After fine-tuning or downloading the fine-tuned checkpoints, the following notebook should be run for testing.

- `src/model/test_functions.ipynb`: This notebook is for testing function_level approach

- `src/model/test_slices.ipynb`: This notebook is for testing slice_level approach


# How to convert function to slices and build our dataset?

> During running these steps, it may require to make several directory/folder manually. Please go through each file and you will easily understand which directory is to build.

### Requirements
>  py2neo-2.0, python-joern-0.3.1, py-igraph and joern 0.3.1（jdk 1.7）, neo4j 2.1.5

### Data Preprocess
1. Download Devign and MSR data by the following way.
    
    - Devign: Download `train.jsonl, test.jsonl and valid.jsonl` from the link: [LINK](https://iastate.box.com/s/mpppv5bvjn4xvboq577w7sslbjlua40q)
    - MSR: Download `MSR_data_cleaned.csv` from the link: [LINK](https://iastate.box.com/s/f5mv33agglgrju0uyztsxotiuo0oxc9m)

2. This step is to write each functions/programs into files inside a directory so that joern can be run in that directory. Besides, this will also create diff information and save into jsonl files. For this step, run the following scripts or notebook:

    Devign
    - src/pata_preprocess/Devign/patch_downloader.ipynb:  This file will add the diff information to the jsonal file and create a new file for each split(e.g test_diff.jsonl). Before running this script, you must copy the FFmpeg and qemu github repo so that this script can access the commit of these projects.
    - src/pata_preprocess/Devign/write_function_file.py: Run this script to write the sample program in files. All files will be stored inside `program_data` folder. The `program_data` folder should be created manually before running this script.

    MSR

    - src/pata_preprocess/MSR/write_function_file.py: This script will write the programs into files inside `program_data` folder as like as devign dataset. This will also create jsonl files(vul.jsonl, non_vul.jsonl) with diff information. 

### Generating Slices 

> Run the following scripts one by one for both Devign and MSR. 

1. src/data_preprocess/joern_runner.py: This file is used to parse source code: the input is source code files directory, and the output is a directory named `.joernIndex`. Then, the neo4j server should be run after setting the path of `.joernIndex` as the neo4j database.

2. src/src2slice/get_cfg_relation.py: This file is used to get CFG graphs of functions using joern tool. The input is output of the first step, and the outputs are stored with folders in cfg_db. 

3. src/src2slice/complete_PDG.py: This file is used to get PDG graph of functions. The inputs are files in cfg_db, and the outputs are stored with folders in pdg_db.

4. src/src2slice/access_db_operate.py: This file is used to get the call graph of functions. The inputs are files in pdg_db, and the outputs are stored with folders in dict_call2cfgNodeID_funcID.

5. src/src2slice/points_get.py: This file is used to get four kinds of SyVCs. The inputs are files in dict_call2cfgNodeID_funcID, and the outputs are four kinds of SyVCs.

6. src/src2slice/extract_df.py: This file is used to extract slices. The inputs are files generated by points_get.py, and the outputs are slice files.


### Labeling Slices and Combining Dataset

> Run the following scripts/notebook:

1. src/src2slice/label_analysis_Devign.ipynb: This script is used to label each sample of Devign dataset. Some vulnerable samples are removed if the diff information is empty.

2. src/src2slice/label_analysis_MSR.ipynb: This script is used to label each sample of MSR. Some vulnerable samples  are removed if its diff information is empty.

3. src/src2slice/merge_dataset.ipynb: This file is used to build the final dataset from the Devign and MSR. Two different folders are created: `func_jsonal` and `slice_jsonal`. 
